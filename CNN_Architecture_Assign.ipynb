{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question1. What is the role of filters and feature maps in Convolutional Neural\n",
        "Network (CNN)?\n",
        "\n",
        "    -> In a Convolutional Neural Network (CNN), filters (also called kernels) are small matrices that slide over the input image to detect specific patterns like edges, textures, or shapes. Each filter focuses on a unique feature of the image. The result of applying these filters is a feature map, which represents the presence and location of the detected features. As the network goes deeper, feature maps capture increasingly complex patterns, helping the CNN learn hierarchical representations of the data.\n",
        "\n",
        "\n",
        "Question 2: Explain the concepts of padding and stride in CNNs(Convolutional NeuralNetwork). How do they affect the output dimensions of feature maps?\n",
        "\n",
        "    -> Padding in CNNs involves adding extra pixels (usually zeros) around the input image to control the spatial dimensions of the output feature map. It helps preserve edge information and prevents reduction in size after convolution. Stride refers to the number of pixels by which the filter moves across the input image; a larger stride reduces the output size, while a stride of one keeps it closer to the input dimensions. Together, padding and stride determine how much the feature map shrinks or retains spatial resolution during convolution.\n",
        "\n",
        "\n",
        "Question 3: Define receptive field in the context of CNNs. Why is it important for deep architectures?    \n",
        "\n",
        "    -> The receptive field in a CNN refers to the specific region of the input image that a particular neuron in a feature map responds to. It represents how much of the input contributes to that neuron’s activation. In deeper architectures, the receptive field increases with each layer, allowing the network to capture more complex and global patterns rather than just local details. This expansion is crucial for understanding higher-level features like shapes, objects, and overall image context.\n",
        "\n",
        "\n",
        "Question 4: Discuss how filter size and stride influence the number of parameters in a CNN.\n",
        "\n",
        "    -> The filter size directly affects the number of parameters in a CNN because each filter contains weights equal to its area multiplied by the number of input channels. Larger filters therefore lead to more parameters and higher computational cost. The stride, on the other hand, controls how much the filter moves across the input — it doesn’t change the number of parameters but affects how many positions (and hence activations) are computed. A larger stride reduces the number of output activations, decreasing the overall computation while keeping the parameter count constant.\n",
        "\n",
        "\n",
        "Question 5: Compare and contrast different CNN-based architectures like LeNet,\n",
        "AlexNet, and VGG in terms of depth, filter sizes, and performance.\n",
        "\n",
        "    -> LeNet is one of the earliest CNN architectures, relatively shallow with about 5–7 layers, using small filters (mostly 5×5) and designed for simple tasks like digit recognition. AlexNet is deeper, with 8 layers, larger filters in the initial layers, and introduced ReLU activation and dropout, achieving breakthrough performance on large-scale datasets like ImageNet. VGG goes even deeper, with 16–19 layers (11×11, 5×5), and standardizes the use of small 3×3 filters stacked in multiple layers, leading to improved feature extraction and accuracy at the cost of higher computational requirements.\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "jHV507_DaYp_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEVlPRCLaS1v",
        "outputId": "c2744a9d-9429-4c7d-c7cb-e9ad26074cd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 32ms/step - accuracy: 0.8789 - loss: 0.4039 - val_accuracy: 0.9800 - val_loss: 0.0681\n",
            "Epoch 2/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 33ms/step - accuracy: 0.9834 - loss: 0.0535 - val_accuracy: 0.9898 - val_loss: 0.0374\n",
            "Epoch 3/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 31ms/step - accuracy: 0.9887 - loss: 0.0346 - val_accuracy: 0.9912 - val_loss: 0.0330\n",
            "Epoch 4/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 31ms/step - accuracy: 0.9917 - loss: 0.0264 - val_accuracy: 0.9910 - val_loss: 0.0315\n",
            "Epoch 5/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 30ms/step - accuracy: 0.9943 - loss: 0.0179 - val_accuracy: 0.9888 - val_loss: 0.0358\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9860 - loss: 0.0408\n",
            "Test Accuracy: 0.9895\n"
          ]
        }
      ],
      "source": [
        "# Question 6: Using keras, build and train a simple CNN model on the MNIST dataset from scratch. Include code for module creation, compilation, training, and evaluation.\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\n",
        "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "\n",
        "# CNN model\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.1)\n",
        "\n",
        "# Evaluation\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7: Load and preprocess the CIFAR-10 dataset using Keras, and create a CNN model to classify RGB images. Show your preprocessing and architecture.\n",
        "\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values\n",
        "x_train, x_test = x_train.astype('float32') / 255.0, x_test.astype('float32') / 255.0\n",
        "\n",
        "# One-hot encoding\n",
        "y_train, y_test = to_categorical(y_train, 10), to_categorical(y_test, 10)\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=64, validation_split=0.1)\n",
        "\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uEoMAaHgFbN",
        "outputId": "9b9da649-ca37-4535-8e7f-5baec1d41482"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 63ms/step - accuracy: 0.3459 - loss: 1.7778 - val_accuracy: 0.5346 - val_loss: 1.2997\n",
            "Epoch 2/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 70ms/step - accuracy: 0.5580 - loss: 1.2439 - val_accuracy: 0.6232 - val_loss: 1.0908\n",
            "Epoch 3/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 60ms/step - accuracy: 0.6279 - loss: 1.0520 - val_accuracy: 0.6510 - val_loss: 0.9979\n",
            "Epoch 4/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 68ms/step - accuracy: 0.6758 - loss: 0.9225 - val_accuracy: 0.6366 - val_loss: 1.0425\n",
            "Epoch 5/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 71ms/step - accuracy: 0.7083 - loss: 0.8419 - val_accuracy: 0.7026 - val_loss: 0.8663\n",
            "Epoch 6/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 74ms/step - accuracy: 0.7386 - loss: 0.7519 - val_accuracy: 0.7156 - val_loss: 0.8427\n",
            "Epoch 7/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 64ms/step - accuracy: 0.7663 - loss: 0.6771 - val_accuracy: 0.7214 - val_loss: 0.8251\n",
            "Epoch 8/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 59ms/step - accuracy: 0.7898 - loss: 0.6012 - val_accuracy: 0.7332 - val_loss: 0.7772\n",
            "Epoch 9/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 60ms/step - accuracy: 0.8040 - loss: 0.5644 - val_accuracy: 0.7320 - val_loss: 0.8001\n",
            "Epoch 10/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 61ms/step - accuracy: 0.8254 - loss: 0.4954 - val_accuracy: 0.7292 - val_loss: 0.8351\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.7201 - loss: 0.8603\n",
            "Test Accuracy: 0.7145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8: Using PyTorch, write a script to define and train a CNN on the MNIST dataset. Include model definition, data loaders, training loop, and accuracy evaluation.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Data preprocessing and loaders\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
        "        self.fc1 = nn.Linear(64*5*5, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64*5*5)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = CNN().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Test Accuracy: {correct/total:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4bHVeRIgdcw",
        "outputId": "e073a059-634e-407e-9693-91027c897c38"
      },
      "execution_count": 5,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 33.3MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.16MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 9.39MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 4.09MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.1585\n",
            "Epoch 2, Loss: 0.0456\n",
            "Epoch 3, Loss: 0.0319\n",
            "Epoch 4, Loss: 0.0233\n",
            "Epoch 5, Loss: 0.0185\n",
            "Test Accuracy: 0.9891\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9: Given a custom image dataset stored in a local directory, write code using Keras ImageDataGenerator to preprocess and train a CNN model.\n",
        "\n",
        "# Import required libraries\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values to [0,1]\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# One-hot encode labels\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Build CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Conv2D(128, (3,3), activation='relu'),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=64, validation_split=0.1)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_NNLOYNeILG",
        "outputId": "c8a33e40-7e3a-472f-e8f4-8623d49903cd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 64ms/step - accuracy: 0.2857 - loss: 1.9077 - val_accuracy: 0.4848 - val_loss: 1.4362\n",
            "Epoch 2/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 60ms/step - accuracy: 0.5021 - loss: 1.3908 - val_accuracy: 0.5746 - val_loss: 1.1865\n",
            "Epoch 3/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 62ms/step - accuracy: 0.5725 - loss: 1.2079 - val_accuracy: 0.6198 - val_loss: 1.0444\n",
            "Epoch 4/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 62ms/step - accuracy: 0.6180 - loss: 1.0913 - val_accuracy: 0.6722 - val_loss: 0.9446\n",
            "Epoch 5/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 56ms/step - accuracy: 0.6544 - loss: 0.9969 - val_accuracy: 0.6938 - val_loss: 0.8917\n",
            "Epoch 6/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 56ms/step - accuracy: 0.6720 - loss: 0.9274 - val_accuracy: 0.6958 - val_loss: 0.8619\n",
            "Epoch 7/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 56ms/step - accuracy: 0.6954 - loss: 0.8736 - val_accuracy: 0.6964 - val_loss: 0.8892\n",
            "Epoch 8/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 58ms/step - accuracy: 0.7073 - loss: 0.8350 - val_accuracy: 0.7248 - val_loss: 0.8018\n",
            "Epoch 9/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 58ms/step - accuracy: 0.7358 - loss: 0.7561 - val_accuracy: 0.7138 - val_loss: 0.8357\n",
            "Epoch 10/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 57ms/step - accuracy: 0.7457 - loss: 0.7255 - val_accuracy: 0.7230 - val_loss: 0.8093\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.7146 - loss: 0.8376\n",
            "Test Accuracy: 0.7148\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working on a web application for a medical imaging startup. Your task is to build and deploy a CNN model that classifies chest X-ray images into “Normal” and “Pneumonia” categories. Describe your end-to-end approach from data preparation and model training to deploying the model as a web app using Streamlit.\n",
        "\n",
        "    -> To build and deploy a CNN-based chest X-ray classifier, I would begin with data preparation, collecting and organizing images into “Normal” and “Pneumonia” folders. Using Keras’ ImageDataGenerator, I would normalize pixel values and apply augmentations like rotation, zoom, and flip to prevent overfitting. Next, I would design a CNN model (e.g., Conv2D → MaxPooling → Dense → Softmax) or use a pretrained model like VGG16 for better accuracy. After compiling with the Adam optimizer and binary cross-entropy loss, I would train and validate the model on the dataset.\n",
        "\n",
        "    Once trained, I’d save the model as an .h5 file and create a Streamlit web app where users can upload X-ray images. The app would load the saved model, preprocess the uploaded image, and display the prediction (“Normal” or “Pneumonia”) along with confidence scores. Finally, I’d deploy the app using Streamlit Cloud, Render, or AWS, ensuring it includes a clean UI, proper error handling, and an intuitive workflow for medical professionals or end users."
      ],
      "metadata": {
        "id": "gGfpkks7f6Hu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "afrxJSqrgC6G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}