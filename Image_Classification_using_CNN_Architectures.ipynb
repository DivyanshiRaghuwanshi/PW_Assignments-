{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is a Convolutional Neural Network (CNN), and how does it differ from traditional fully connected neural networks in terms of architecture and performance on image data?\n",
        "\n",
        "    -> A Convolutional Neural Network (CNN) is a deep learning architecture designed to automatically and adaptively learn spatial hierarchies of features from images. Unlike traditional fully connected neural networks where each neuron connects to every input, CNNs use convolutional layers with local receptive fields and shared weights to capture spatial patterns efficiently. This makes them highly effective for image data, reducing the number of parameters and improving performance in tasks like image classification, detection, and recognition.\n",
        "\n",
        "Question 2: Discuss the architecture of LeNet-5 and explain how it laid the foundation for modern deep learning models in computer vision. Include references to its original research paper.\n",
        "\n",
        "    -> LeNet-5, introduced by Yann LeCun et al. in 1998 (“Gradient-Based Learning Applied to Document Recognition,” Proceedings of the IEEE), is one of the earliest CNN architectures designed for handwritten digit recognition. It consists of seven layers, including convolutional, subsampling (pooling), and fully connected layers, followed by a softmax output layer. LeNet-5 demonstrated how convolutional and pooling operations could efficiently extract hierarchical features from images, laying the foundation for modern CNNs like AlexNet, VGG, and ResNet by showcasing the power of deep feature learning in computer vision tasks.\n",
        "\n",
        "\n",
        "Question 3: Compare and contrast AlexNet and VGGNet in terms of design principles,number of parameters, and performance. Highlight key innovations and limitations of each.\n",
        "\n",
        "    -> AlexNet, introduced by Krizhevsky et al. in 2012, was a breakthrough CNN architecture that popularized deep learning by achieving record performance on ImageNet. It used five convolutional layers, ReLU activations, dropout, and GPU training to significantly outperform traditional methods. VGGNet, proposed by Simonyan and Zisserman in 2014, built upon AlexNet by using a deeper architecture with 16–19 layers and smaller 3×3 convolution filters for better feature extraction. While VGGNet achieved higher accuracy with a more uniform design, it required far more parameters and computational resources than AlexNet, making it slower and more memory-intensive despite its improved generalization.\n",
        "\n",
        "\n",
        "Question 4: What is transfer learning in the context of image classification? Explain how it helps in reducing computational costs and improving model performance with limited data.   \n",
        "\n",
        "    -> Transfer learning in image classification involves using a pre-trained model, such as VGG, ResNet, or Inception, trained on large datasets like ImageNet, and fine-tuning it for a new but related task. This approach allows the model to reuse previously learned visual features, significantly reducing the need for large labeled datasets and extensive training time. By leveraging learned representations, transfer learning improves model performance, especially when data is scarce, while lowering computational costs since only the final layers are retrained instead of the entire network.\n",
        "\n",
        "\n",
        "Question 5: Describe the role of residual connections in ResNet architecture. How do they address the vanishing gradient problem in deep CNNs?   \n",
        "\n",
        "    -> Residual connections in the ResNet architecture enable the network to learn identity mappings by adding shortcut links that skip one or more layers. These connections allow gradients to flow directly through the network during backpropagation, effectively mitigating the vanishing gradient problem that occurs in very deep CNNs. By letting layers learn residual functions rather than complete transformations, ResNet ensures easier optimization, enabling the successful training of extremely deep networks with hundreds of layers while maintaining high accuracy and stability.\n",
        "    "
      ],
      "metadata": {
        "id": "B0Klkp0w9hSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install TensorFlow into this notebook's kernel\n",
        "# If you have a supported NVIDIA GPU and drivers, you can replace the second line with:\n",
        "# %pip install tensorflow[and-cuda]\n",
        "%pip install --upgrade pip\n",
        "%pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cn3nIMqOGQzO",
        "outputId": "f1a70a63-6724-4bcb-dee3-724cde2d9360"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "Downloading pip-25.2-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-25.2\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.75.1)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick environment check after install\n",
        "import sys, site\n",
        "import tensorflow as tf\n",
        "print(\"Python executable:\", sys.executable)\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"Site-packages:\")\n",
        "for p in site.getsitepackages():\n",
        "    print(\" -\", p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZBhWHdsGVIe",
        "outputId": "e7f2f069-74cb-430e-f835-d38c3cec67d6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python executable: /usr/bin/python3\n",
            "TensorFlow version: 2.19.0\n",
            "Site-packages:\n",
            " - /usr/local/lib/python3.12/dist-packages\n",
            " - /usr/lib/python3/dist-packages\n",
            " - /usr/lib/python3.12/dist-packages\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 6: Implement the LeNet-5 architectures using Tensorflow or PyTorch to classify the MNIST dataset. Report the accuracy and training time.\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = (x_train.astype(\"float32\") / 255.0)[..., np.newaxis]\n",
        "x_test = (x_test.astype(\"float32\") / 255.0)[..., np.newaxis]\n",
        "\n",
        "# Pad to 32x32 for a faithful LeNet-5 input size (original used 32x32)\n",
        "x_train = np.pad(x_train, ((0, 0), (2, 2), (2, 2), (0, 0)), mode=\"constant\")\n",
        "x_test = np.pad(x_test, ((0, 0), (2, 2), (2, 2), (0, 0)), mode=\"constant\")\n",
        "\n",
        "# Define LeNet-5 (tanh activations + average pooling)\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(6, kernel_size=(5, 5), activation='tanh', input_shape=(32, 32, 1)),\n",
        "    layers.AveragePooling2D(pool_size=(2, 2), strides=2),\n",
        "    layers.Conv2D(16, kernel_size=(5, 5), activation='tanh'),\n",
        "    layers.AveragePooling2D(pool_size=(2, 2), strides=2),\n",
        "    # Original LeNet-5 has a conv layer that produces 120 feature maps with 5x5 kernels\n",
        "    layers.Conv2D(120, kernel_size=(5, 5), activation='tanh'),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(84, activation='tanh'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "start_time = time.time()\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=10,\n",
        "    batch_size=128,\n",
        "    validation_split=0.1,\n",
        "    verbose=1\n",
        ")\n",
        "train_time = time.time() - start_time\n",
        "\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"Test Accuracy: {test_acc * 100:.2f}%\")\n",
        "print(f\"Training Time: {train_time:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kww5RVM6Ger0",
        "outputId": "39161c88-9e5d-4c6c-92a7-7a9798b4c5b2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 65ms/step - accuracy: 0.7264 - loss: 0.9264 - val_accuracy: 0.9492 - val_loss: 0.1799\n",
            "Epoch 2/10\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 65ms/step - accuracy: 0.9385 - loss: 0.2042 - val_accuracy: 0.9672 - val_loss: 0.1139\n",
            "Epoch 3/10\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 66ms/step - accuracy: 0.9598 - loss: 0.1325 - val_accuracy: 0.9767 - val_loss: 0.0858\n",
            "Epoch 4/10\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 80ms/step - accuracy: 0.9696 - loss: 0.0991 - val_accuracy: 0.9787 - val_loss: 0.0714\n",
            "Epoch 5/10\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 66ms/step - accuracy: 0.9752 - loss: 0.0799 - val_accuracy: 0.9817 - val_loss: 0.0630\n",
            "Epoch 6/10\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 83ms/step - accuracy: 0.9797 - loss: 0.0672 - val_accuracy: 0.9830 - val_loss: 0.0575\n",
            "Epoch 7/10\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 73ms/step - accuracy: 0.9825 - loss: 0.0581 - val_accuracy: 0.9837 - val_loss: 0.0536\n",
            "Epoch 8/10\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 78ms/step - accuracy: 0.9846 - loss: 0.0512 - val_accuracy: 0.9853 - val_loss: 0.0505\n",
            "Epoch 9/10\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 66ms/step - accuracy: 0.9865 - loss: 0.0457 - val_accuracy: 0.9858 - val_loss: 0.0480\n",
            "Epoch 10/10\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 63ms/step - accuracy: 0.9879 - loss: 0.0412 - val_accuracy: 0.9867 - val_loss: 0.0460\n",
            "Test Accuracy: 98.40%\n",
            "Training Time: 313.07 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Strategy for X-ray classification with limited labels (normal, pneumonia, COVID-19)\n",
        "\n",
        " Answer:\n",
        "Approach: Transfer learning with a strong pretrained backbone (e.g., ResNet50, EfficientNet-B0/B3)\n",
        "trained on ImageNet, optionally further pretrain on large chest X-ray datasets (e.g., CheXpert)\n",
        " using self-supervised or weakly supervised methods if labels are scarce.\n",
        " - Data handling: Use class-balanced sampling, heavy augmentations (random rotation, shift, CLAHE/contrast,\n",
        "   random cropping, cutout), and ensure strict patient-wise splits.\n",
        " - Fine-tuning: Freeze backbone → train new head; then unfreeze last block(s) with low LR; use early stopping\n",
        "  and model checkpointing; evaluate with stratified k-fold if dataset small.\n",
        "- Loss/metrics: Focal loss or class-weighted cross-entropy; metrics: AUC, sensitivity/specificity, F1.\n",
        "- Explainability: Integrate Grad-CAM for clinician trust and error analysis.\n",
        "- Deployment: Export to ONNX/TF SavedModel; serve behind an API with GPU/CPU autoscaling; add input QA checks,\n",
        " DICOM de-identification, audit logging; monitor drift with periodic re-evaluation and threshold tuning.\n",
        "- Privacy & compliance: HIPAA-safe storage, role-based access, and human-in-the-loop review for edge cases.\n",
        "\n",
        "print(\"Strategy outlined for production-ready, data-limited medical X-ray classification.\")"
      ],
      "metadata": {
        "id": "RyBCY4LeHS4v"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QS16fTvaHtqw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}