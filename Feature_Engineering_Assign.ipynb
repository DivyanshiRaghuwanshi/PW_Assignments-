{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a parameter?\n",
        "    -> A parameter in machine learning refers to a configuration variable that is internal to the model and whose value is estimated from the data during training. Examples include the weights and biases in neural networks or coefficients in linear regression. These are learned automatically and determine how the model makes predictions.\n",
        "\n",
        "2. What is correlation?  What does negative correlation mean?\n",
        "    -> Correlation is a statistical measure that indicates the degree to which two variables move in relation to each other. In feature engineering, it helps identify relationships between features, where a positive correlation means both variables increase together and a negative correlation means one increases while the other decreases. High correlation between features can lead to redundancy, so it's often used to reduce multicollinearity.\n",
        "\n",
        "    Negative correlation means that as one variable increases, the other decreases, indicating an inverse relationship between the two. In the context of machine learning, if two features have a strong negative correlation, one may be reduced or removed during feature selection to avoid redundancy or overfitting.\n",
        "\n",
        "3. Define Machine Learning. What are the main components in Machine Learning?\n",
        "    -> Machine Learning is a subset of artificial intelligence that enables systems to learn from data and make predictions or decisions without being explicitly programmed. The main components in machine learning are the dataset (input data), features (independent variables), model/algorithm (used to learn patterns), loss function (to measure prediction error), and optimizer (to update parameters and minimize error).\n",
        "\n",
        "4. How does loss value help in determining whether the model is good or not?\n",
        "    -> The loss value quantifies how far off a model's predictions are from the actual outcomes, acting as a direct measure of model performance. A lower loss generally indicates that the model is making more accurate predictions, while a high loss signals poor learning or incorrect predictions. During training, monitoring the loss helps in adjusting model parameters to improve accuracy.\n",
        "\n",
        "5. What are continuous and categorical variables?\n",
        "    -> Continuous variables are numerical values that can take on any value within a range, such as height, weight, or temperature, and are typically used in regression tasks. Categorical variables represent discrete groups or categories, like gender, color, or country, and are often used in classification problems. Both types require different preprocessing techniques during feature engineering.\n",
        "\n",
        "6. How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "    -> Categorical variables in machine learning are handled by converting them into a numerical format since most models require numeric input. Common techniques include Label Encoding, which assigns each category a unique number, and One-Hot Encoding, which creates binary columns for each category. Other methods like Target Encoding or Frequency Encoding are also used based on the nature of the data and the model.\n",
        "\n",
        "7. What do you mean by training and testing a dataset?\n",
        "    -> Training a dataset means using a portion of the data to teach the machine learning model to recognize patterns and relationships, while testing a dataset involves evaluating the model’s performance on unseen data to check how well it generalizes. Typically, data is split into training and testing sets to ensure the model isn’t just memorizing but actually learning to make accurate predictions.                      \n",
        "\n",
        "8. What is sklearn.preprocessing?\n",
        "    -> sklearn.preprocessing is a module in Scikit-learn that provides various tools to scale, transform, and encode data before feeding it into machine learning models. It includes functions like StandardScaler, MinMaxScaler, LabelEncoder, and OneHotEncoder, which help normalize features and handle categorical variables efficiently. Preprocessing ensures that the data is in the right format and scale for optimal model performance.\n",
        "\n",
        "9. What is a Test set?\n",
        "    -> A test set is a portion of the dataset that is not used during model training but is reserved to evaluate the model's performance on unseen data. It helps check how well the model generalizes beyond the training data and provides an unbiased estimate of its accuracy, precision, or other performance metrics.\n",
        "\n",
        "10.  How do we split data for model fitting (training and testing) in Python?\n",
        " How do you approach a Machine Learning problem?\n",
        "    -> To split data for model fitting in Python, we use train_test_split from sklearn.model_selection, which divides the dataset into training and testing sets, typically with 70–80% for training and 20–30% for testing. To approach a machine learning problem, start by understanding the problem and data, then preprocess the data, select relevant features, choose and train a model, evaluate its performance, and fine-tune it before deploying.\n",
        "\n",
        "11. Why do we have to perform EDA before fitting a model to the data?\n",
        "    -> We perform Exploratory Data Analysis (EDA) before fitting a model to understand the underlying structure, patterns, and anomalies in the dataset. EDA helps in identifying missing values, outliers, correlations, and feature distributions, which guide better preprocessing and feature selection. It ensures the model is built on clean, relevant, and meaningful data, ultimately improving accuracy and reliability.\n",
        "\n",
        "12. What is correlation?\n",
        "    -> Correlation is a statistical measure that describes the strength and direction of a linear relationship between two variables. It ranges from -1 to 1, where values close to 1 indicate strong positive correlation, values near -1 indicate strong negative correlation, and values around 0 suggest no linear relationship. In feature engineering, correlation helps identify redundant or highly related features.\n",
        "\n",
        "13. What does negative correlation mean?\n",
        "    -> Negative correlation means that as one variable increases, the other decreases, indicating an inverse relationship between the two. In the context of machine learning, if two features have a strong negative correlation, one may be reduced or removed during feature selection to avoid redundancy or overfitting.\n",
        "\n",
        "14.  How can you find correlation between variables in Python?\n",
        "    -> You can find correlation between variables in Python using the .corr() method on a pandas DataFrame, which computes pairwise correlation coefficients between numerical columns. To visualize correlations, you can use heatmaps with libraries like seaborn (sns.heatmap(df.corr())) to quickly identify strongly related features.\n",
        "\n",
        "15. What is causation? Explain difference between correlation and causation with an example.\n",
        "    -> Causation means that one variable directly affects or causes a change in another, whereas correlation only indicates that two variables move together but doesn't prove one causes the other. For example, ice cream sales and drowning incidents may be positively correlated (both increase in summer), but ice cream doesn't cause drowning—weather is the actual cause, showing correlation but not causation.\n",
        "\n",
        "16. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "    -> An optimizer is an algorithm that adjusts a machine learning model’s parameters (like weights) to minimize the loss function and improve accuracy during training. Common optimizers include SGD (Stochastic Gradient Descent), which updates weights using gradients on mini-batches, Adam, which combines momentum and adaptive learning rates for faster convergence, and RMSprop, which adapts learning rates based on recent gradients. For example, using optimizer = torch.optim.Adam(model.parameters()) in PyTorch applies the Adam optimizer.\n",
        "\n",
        "17.  What is sklearn.linear_model ?\n",
        "    -> sklearn.linear_model is a module in Scikit-learn that provides classes and functions for implementing linear models, such as Linear Regression, Logistic Regression, Ridge, and Lasso. These models are used for both regression and classification tasks and allow regularization and custom loss functions. For example, LinearRegression() can be used to fit a line to predict continuous values based on input features.\n",
        "\n",
        "18. What does model.fit() do? What arguments must be given?\n",
        "    -> The model.fit() function trains a machine learning model on the provided training data, learning the relationship between input features and target values. It usually requires at least two arguments: X (feature matrix) and y (target vector). For example, model.fit(X_train, y_train) fits the model to the training data so it can make predictions.\n",
        "\n",
        "19. What does model.predict() do? What arguments must be given?\n",
        "    -> The model.predict() function is used to make predictions on new or unseen data using a trained machine learning model. It requires a single argument: the feature matrix X for which you want predictions, such as model.predict(X_test). The output is an array of predicted values or class labels depending on the type of model.\n",
        "\n",
        "20. What are continuous and categorical variables?\n",
        "    -> Continuous variables are numeric values that can take any value within a range, such as age, income, or temperature, and are typically used in regression tasks. Categorical variables represent distinct categories or groups, like gender, product type, or color, and are commonly used in classification problems. Both types influence how data is preprocessed and modeled.\n",
        "\n",
        "21. What is feature scaling? How does it help in Machine Learning?\n",
        "    -> Feature scaling is the process of transforming features to a common scale, usually so they fall within a specific range like 0–1 or have zero mean and unit variance. It helps in machine learning by ensuring that algorithms sensitive to feature magnitude—like gradient descent, KNN, and SVM—work efficiently and converge faster, preventing bias toward larger-valued features.\n",
        "\n",
        "22.  How do we perform scaling in Python?\n",
        "    -> Scaling in Python is commonly performed using StandardScaler or MinMaxScaler from sklearn.preprocessing. StandardScaler() standardizes features to have zero mean and unit variance, while MinMaxScaler() scales data to a given range (typically 0 to 1). For example: scaler = StandardScaler(); X_scaled = scaler.fit_transform(X) applies standard scaling to your feature set X.\n",
        "\n",
        "23. What is sklearn.preprocessing?\n",
        "    -> sklearn.preprocessing is a module in Scikit-learn that provides various tools to scale, transform, and encode data before feeding it into machine learning models. It includes functions like StandardScaler, MinMaxScaler, LabelEncoder, and OneHotEncoder, which help normalize features and handle categorical variables efficiently. Preprocessing ensures that the data is in the right format and scale for optimal model performance.\n",
        "\n",
        "24. How do we split data for model fitting (training and testing) in Python?\n",
        "    -> In Python, data is split for model fitting using train_test_split from sklearn.model_selection, which randomly divides the dataset into training and testing sets. Typically, you import it as from sklearn.model_selection import train_test_split and then use it like X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2), where test_size=0.2 means 20% of data is used for testing.\n",
        "\n",
        "25.  Explain data encoding?\n",
        "    -> Data encoding is the process of converting categorical variables into numerical form so they can be used in machine learning models, which require numeric input. Common encoding techniques include Label Encoding (assigns integers to categories) and One-Hot Encoding (creates binary columns for each category), ensuring the model can interpret and learn from non-numeric features effectively.\n",
        "\n",
        "                                                             "
      ],
      "metadata": {
        "id": "fJf2XVM9NYcm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KE1M-K_oNL7H"
      },
      "outputs": [],
      "source": []
    }
  ]
}